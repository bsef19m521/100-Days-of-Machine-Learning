# 100-Days-of-Machine-Learning

## Day 89: Building RNNs and Variants of RNNs

**Learning Agenda of Day 89 is :**

- Practice building and training RNNs for text generation, language translation and speech recognition tasks using popular datasets such as IMDB, Wikipedia and CommonVoice: 
	- Practicing building and training RNNs for different tasks using popular datasets is important for gaining hands-on experience and understanding how to apply RNNs to real-world problems. 
	- It also provides a way to benchmark your progress and compare your results with other models.

- Introduction to variants of RNNs such as LSTM and GRU: 
	- RNNs have several variants, such as LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), which are designed to better handle the problem of vanishing gradients, a problem that occurs when training traditional RNNs. 
	- Understanding the differences between these variants and how they work will help you choose the right type of RNN for different tasks.

